seed: -1
torch_deterministic: True

clip_observations: 5.0
clip_actions: 1.0
model_dir: 

origin_shape: 37 #env origin
proprioception_shape: 27  #proprio = q + pos 
encoded_observation_shape: 35 # encoded = proprio + latent
env_shape: 14 #env_shape = q + obj
latent_shape: 8

encoder_model_path: "/home/nathan/VisualTactile/run/encoder_model_2000.pt"
rl_model_path: "/home/nathan/VisualTactile/run/model_2000.pt"

policy: # only works for MlpPolicy right now
  pi_hid_sizes: [512, 256, 128]
  vf_hid_sizes: [512, 256, 128]
  activation: elu # can be elu, relu, selu, crelu, lrelu, tanh, sigmoid

env_encoder:
  e_hid_sizes: [256,128]
  e_activation: elu
  
learn:
  test: False
  resume: 0
  save_interval: 400 # check for potential saves every this many iterations
  print_log: True

  # rollout params
  max_iterations: 8000

  # training params
  cliprange: 0.2
  ent_coef: 0
  nsteps: 8
  noptepochs: 5
  nminibatches: 4 # this is per agent
  max_grad_norm: 1
  optim_stepsize: 3.e-4 # 3e-4 is default for single agent training with constant schedule
  schedule: adaptive # could be adaptive or linear or fixed
  desired_kl: 0.016
  gamma: 0.96
  lam: 0.95
  init_noise_std: 0.8

  log_interval: 5
  asymmetric: False